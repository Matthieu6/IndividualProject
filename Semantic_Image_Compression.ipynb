{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMnbc67gyRCfV2JSynFu7Hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthieu6/IndividualProject/blob/main/Semantic_Image_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Semantic Image Compression Scheme"
      ],
      "metadata": {
        "id": "FizhyXBh45wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code shows the final image compression scheme, created by taking open-source image captioning and image generation models to construct an encoder-decoder architecture.\n",
        "\n",
        "*If the following model does not work due to limited RAM available, make sure to run the Encoder and Decoder sections separately.*"
      ],
      "metadata": {
        "id": "U41cwae14_Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder (image captioning)"
      ],
      "metadata": {
        "id": "M97blNjoBVh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate the image captioning model (BLIP-2) installing any packages."
      ],
      "metadata": {
        "id": "WMKu7yIbBjg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install salesforce-lavis\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from lavis.models import load_model_and_preprocess\n",
        "\n",
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device)\n",
        "\n",
        "vis_processors.keys()"
      ],
      "metadata": {
        "id": "B9f_dHFhBd4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs the code to create a caption from the input image. This will act as the data transferred from one location to another."
      ],
      "metadata": {
        "id": "7-fAHyzjBt2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#change name to the file you would like to complete semantic image compression to\n",
        "raw_image = Image.open(\"input.jpg\").convert('RGB')\n",
        "\n",
        "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
        "# Generate text from the image\n",
        "gen_output = model.generate({\"image\": image}, min_length=16, max_length=64)\n",
        "# Assume the output is directly the text. If it's in a different format, you'll need to adjust this part\n",
        "generated_text = gen_output\n",
        "\n"
      ],
      "metadata": {
        "id": "YcKOhMp9Buam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder (image Generation)"
      ],
      "metadata": {
        "id": "o5GNj4ftBZu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate the image generation model (StableSDXL) installing any packages."
      ],
      "metadata": {
        "id": "wardlPNwCVtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "repo = \"ByteDance/SDXL-Lightning\"\n",
        "ckpt = \"sdxl_lightning_8step_unet.safetensors\" # Use the correct ckpt for your step setting!\n",
        "\n",
        "# Load model.\n",
        "unet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\n",
        "unet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
        "\n",
        "# Ensure sampler uses \"trailing\" timesteps.\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")"
      ],
      "metadata": {
        "id": "frnErvxyCWL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates an image from the image caption generated in the encoder architecture section"
      ],
      "metadata": {
        "id": "aBStxOIrCawK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the caption text as input for image generation\n",
        "pipe(generated_text, num_inference_steps=8, guidance_scale=7).images[0].save(\"output.png\")"
      ],
      "metadata": {
        "id": "FS45Var-CZxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will be a .png file, representing the input image passed through the semantic image compression scheme."
      ],
      "metadata": {
        "id": "8-hU1E78Cx6e"
      }
    }
  ]
}